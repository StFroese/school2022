{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to CTA Data\n",
    "\n",
    "\n",
    "Notebook stolen and modified from https://github.com/tudo-astroparticlephysics/machine-learning-lecture/blob/main/smd_boosting.ipynb\n",
    "\n",
    "main author: Maximilian Nöthe\n",
    "\n",
    "additional authors: Thomas Vuillaume, Michaël Dell'aiera, Martino Sorbaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:05:27.643365Z",
     "start_time": "2018-11-27T15:05:26.177785Z"
    }
   },
   "outputs": [],
   "source": [
    "import ml_func as ml\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:05:27.643365Z",
     "start_time": "2018-11-27T15:05:26.177785Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "ml.set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Complete Example\n",
    "\n",
    "Below we load a dataset containing data from simulated CTA Observations.\n",
    "\n",
    "<img width=\"100%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/CTA_Telescopes_in_Southern_Hemisphere.jpg/1280px-CTA_Telescopes_in_Southern_Hemisphere.jpg\" />   \n",
    "\n",
    "We will perform the typical steps to build and evaluate a classifier.\n",
    "\n",
    "0. Understand where your data comes from\n",
    "\n",
    "1. Preprocessing\n",
    "    * Drop Constant Values,\n",
    "    * Handle Missing Data \n",
    "    * Feature Generation\n",
    "\n",
    "2. Splitting\n",
    "    \n",
    "    * Split your data into training and evaluation sets\n",
    "    \n",
    "3. Training \n",
    "    \n",
    "    * Train your classifier of choice.\n",
    "    \n",
    "4. Evaluation\n",
    "    \n",
    "    * Evaluate the performance on the test data set.\n",
    "    * If not good enough, go back to step 1 \n",
    "    \n",
    "5. Physics\n",
    "    \n",
    "    * Check whether your data support your hypothesis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get to know your data\n",
    "\n",
    "Cherenkov telescopes record short flashes of light produced by very high energy cosmic rays and photons hitting earths atmosphere.\n",
    "\n",
    "![](https://www.cta-observatory.org/wp-content/uploads/2016/05/cta47.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:00.589316Z",
     "start_time": "2018-11-27T15:07:00.584438Z"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- https://nextcloud.e5.physik.tu-dortmund.de/index.php/s/e7yb2mifGDeyDBN/download -->\n",
    "<video width=\"100%\" controls>\n",
    "  <source src=\"./resources/fact_events.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use machine learning for two tasks in this example. \n",
    "\n",
    " * Train a classifier to distinguish events induced by gamma rays form events induced by cosmic rays\n",
    " * Train a regressor to estimate the energy of the incoming primary particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the array of telescopes\n",
    "\n",
    "array = ml.SubarrayDescription.from_hdf(ml.gamma_filename)\n",
    "array.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data\n",
    "\n",
    "A lot of preprocessing has already happened at this point.\n",
    "\n",
    "* Calibration of Raw Data\n",
    "* Data Reduction from voltage timeseries per pixel to number of photons and mean time for each pixel\n",
    "* Calculation of image features\n",
    "* Geometrical Reconstruction of the Shower Geometry\n",
    "\n",
    "\n",
    "Load data and remove unwanted columns and store the true labels separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = ml.get_gammas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gammas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data types of the columns. We can select non-numeric types and encode them. But in this case we might as well drop them as the attribute is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.075296Z",
     "start_time": "2018-11-27T15:07:02.057255Z"
    }
   },
   "outputs": [],
   "source": [
    "c = gammas.select_dtypes(exclude=['number']).columns\n",
    "print('Removed columns:', c.values)\n",
    "\n",
    "gammas = gammas.drop(c, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot the columns with constant values by looking at the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.529060Z",
     "start_time": "2018-11-27T15:07:02.078743Z"
    }
   },
   "outputs": [],
   "source": [
    "desc = gammas.describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.547778Z",
     "start_time": "2018-11-27T15:07:02.532415Z"
    }
   },
   "outputs": [],
   "source": [
    "c = desc.columns[desc.loc['std'] == 0]\n",
    "print('Removed columns:', c.values)\n",
    "gammas = gammas.drop(c, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop columns where all rows are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = gammas.columns[gammas.count() == 0]\n",
    "print('Removed columns:', c.values)\n",
    "gammas = gammas.drop(c, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing data. (Just delete it in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.594941Z",
     "start_time": "2018-11-27T15:07:02.551401Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(gammas))\n",
    "gammas = gammas.dropna()\n",
    "print(len(gammas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we only loaded simulated gamma-ray showers. Now we do the same for the cosmic ray events. We create a method to perform all preprocessing in one step. We need this several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.608429Z",
     "start_time": "2018-11-27T15:07:02.597366Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \n",
    "    c = df.select_dtypes(exclude=['number']).columns\n",
    "    df = df.drop(c, axis='columns')\n",
    "    \n",
    "    c = df.columns[df.count() == 0]\n",
    "    df = df.drop(c, axis='columns')\n",
    "    \n",
    "    desc = df.describe()\n",
    "    \n",
    "    c = desc.columns[desc.loc['std'] == 0]\n",
    "    df = df.drop(c, axis='columns')\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.467654Z",
     "start_time": "2018-11-27T15:07:02.611649Z"
    }
   },
   "outputs": [],
   "source": [
    "protons = ml.get_protons()\n",
    "protons = preprocess(protons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform feature generation. We use our expert knowledge or intuition to create a new feature by combining existing columns into a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.481076Z",
     "start_time": "2018-11-27T15:07:03.471131Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_generation(df):\n",
    "    df['awesome_feature'] =  df['hillas_intensity'] * (df['hillas_width'] / df['hillas_length'])\n",
    "    \n",
    "    # distance of impact point to the telescope\n",
    "    df['impact'] = np.sqrt(\n",
    "        (df['HillasReconstructor_core_x'] - df['pos_x'])**2\n",
    "        + (df['HillasReconstructor_core_y'] - df['pos_y'])**2\n",
    "    )\n",
    "    return df\n",
    "\n",
    "gammas = feature_generation(gammas)\n",
    "protons = feature_generation(protons)\n",
    "\n",
    "gammas[['awesome_feature', 'impact']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.829801Z",
     "start_time": "2018-11-27T15:07:03.484278Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(-20, 20, 100)\n",
    "# bins = np.logspace(0, 1, 100)\n",
    "# bins = 100\n",
    "bins = np.arange(0, 10) - 0.5\n",
    "bins = np.geomspace(1e3, 30e3, 51)\n",
    "\n",
    "col = 'HillasReconstructor_h_max'\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(gammas[col], bins=bins, histtype='step', lw=2, label='Gammas', density=True)\n",
    "plt.hist(protons[col], bins=bins, histtype='step', lw=2, label='Protons', density=True)\n",
    "plt.xscale('log')\n",
    "plt.xlabel(col)\n",
    "plt.legend()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "You may plot other histogram to get a better grasp at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(gammas['true_core_x'], gammas['true_core_y'], s=1, alpha=0.4)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a regressor to reconstruct the gammas energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select the training features by removing columns with `true_` values coming from the simulation\n",
    "training_features = [name for name in list(gammas.columns) if not name.startswith('true_')]\n",
    "predict_feature = 'true_energy'\n",
    "\n",
    "training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = gammas[training_features]\n",
    "y = gammas[predict_feature]\n",
    "\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DecisionTreeRegressor(max_depth=30)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist2d(y_test, y_predict, bins=np.logspace(-2, 2, 60))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_log_error(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra: How can we improve that regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train a classifier to do gamma/hadron separation\n",
    "\n",
    "Data preparation for gamma/hadron classification\n",
    "\n",
    "Execise: split the data into train / test using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we combine the two datasets into one big matrix and build a label vector $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.895879Z",
     "start_time": "2018-11-27T15:07:03.832960Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.concat([gammas[training_features], protons[training_features]])\n",
    "y = np.concatenate([np.ones(len(gammas)), np.zeros(len(protons))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.983514Z",
     "start_time": "2018-11-27T15:07:03.898708Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier\n",
    "\n",
    "Now we can train any classifier we want on the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:13:04.181685Z",
     "start_time": "2018-11-27T15:13:02.875532Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "rf = DecisionTreeClassifier(max_depth=15, criterion='entropy')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rf.predict(X_test)\n",
    "y_prediction_proba = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Evaluation \n",
    "\n",
    "Check accuracy of the models and other metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.Series(rf.feature_importances_, index=training_features)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "importance.sort_values().tail(20).plot.barh()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:13:04.199205Z",
     "start_time": "2018-11-27T15:13:04.183884Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:13:46.845086Z",
     "start_time": "2018-11-27T15:13:46.501506Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(fpr, tpr, c=thresholds, vmax=1)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.gca().set_aspect(1)\n",
    "plt.plot(fpr, tpr, '--', color='gray', alpha=0.5)\n",
    "plt.text(0.5, 0.5, f'AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}')\n",
    "plt.colorbar()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Redo, using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform steps 3, 4, and 5 in one step using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:08:28.709336Z",
     "start_time": "2018-11-27T15:08:03.738680Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "rf = DecisionTreeClassifier(max_depth=12, criterion='entropy')\n",
    "\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'auc': 'roc_auc',\n",
    "           'recall': 'recall'}\n",
    "\n",
    "results = cross_validate(rf, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:12:14.425262Z",
     "start_time": "2018-11-27T15:12:14.404548Z"
    }
   },
   "outputs": [],
   "source": [
    "auc = results['test_auc']\n",
    "recall = results['test_recall']\n",
    "acc = results['test_acc']\n",
    "\n",
    "print(f'Area under RoC curve: {auc.mean():0.04f} ± {auc.std():0.04f}')\n",
    "print(f'Accuracy:             {acc.mean():0.04f} ± {acc.std():0.04f}')\n",
    "print(f'Recall:               {recall.mean():0.04f} ± {recall.std():0.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Physics\n",
    "\n",
    "Now we could test our model and our hypothesis on real observed data. This part of the analysis is the most time \n",
    "consuming in general. It also requires more data than than this notebook can handle. \n",
    "After careful analysis one can produce an image of the gamma-ray sky\n",
    "\n",
    "<img width=\"60%\" src=\"https://www.mpi-hd.mpg.de/hfm/HESS/hgps/figures/HESS_J1813m126.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Improving Classification\n",
    "\n",
    "\n",
    "### Boosting and AdaBoost\n",
    "\n",
    "Similar to the idea of combining many classifiers through bagging (like we did for the RandomForests) we now \n",
    "train many estimators in a sequential manner. In each iteration the data gets modified slightly using weights $w$\n",
    "for each sample in the training data. In the first iteration the weights are all set to $w=1$\n",
    "\n",
    "In each successive iteration the weights are updated. The samples that were incorrectly classified in the previous \n",
    "iteration get a higher weight. The weights for correctly classified samples get decreases. \n",
    "In other words: We increase the influence/importance of samples that are difficult to classify.\n",
    "\n",
    "Predictions are performed by taking a weighted average of the single predictors.\n",
    "\n",
    "The popular AdaBoost algorithms takes this a step further by optimizing the weight of each separate classifier \n",
    "in the ensemble.\n",
    "The AdaBoost ensemble combines many learners in an iterative way. The learner at iteration $m$ is:\n",
    "\n",
    "$$\n",
    " F_{m}(x)=F_{m-1}(x)+\\gamma _{m}h_{m}(x)\n",
    "$$\n",
    "\n",
    "The choice of $F_0$ is problem specific.\n",
    "\n",
    "Each weak learner produces a prediction $h(x_{m})$ for each sample in the training set. At each iteration $m$ a \n",
    "weak learner is fitted and assigned a coefficient $\\gamma_{m}$ which is found by minimizing:\n",
    "\n",
    "$$\n",
    "\\gamma_m = {\\underset {\\gamma }{\\arg \\min }} \\sum_{i}^{N}E\\bigl(F_{m-1}(x_{i})+\\gamma h(x_{i})\\bigr)\n",
    "$$\n",
    "\n",
    "where $E(F)$ is some error function and $x_i$ is the reweighted data sample.\n",
    "\n",
    "In general this method can work with any classifying method. Traditionally it is being used with very small \n",
    "decision trees. \n",
    "The weights get used to select the split points during the minimization of the loss function in each node\n",
    "\n",
    "$$\n",
    " \\underset{(X, s) \\in \\, \\mathbf{X} \\times {S}}{\\arg \\max} IG(X,Y) =   \\underset{(X, s) \\in \\, \\mathbf{X} \\times {S}}{\\arg \\max} ( H(Y) - H(Y |\\, X) ).\n",
    "$$\n",
    "\n",
    "Below we try AdaBoost on the CTA data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:49:09.247110Z",
     "start_time": "2018-11-27T15:48:58.872812Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    ")\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = ada.predict(X_test)\n",
    "y_prediction_proba = ada.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:49:10.072785Z",
     "start_time": "2018-11-27T15:49:09.249195Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = np.array(list(ada.staged_score(X_test, y_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scores, '.')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:49:10.727863Z",
     "start_time": "2018-11-27T15:49:10.075262Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(fpr, tpr, c=thresholds)\n",
    "plt.plot(fpr, tpr, '--', color='gray', alpha=0.5)\n",
    "plt.text(0.5, 0.5, f'AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting \n",
    "\n",
    "Very similar to AdaBoost. Only this time we change the target label we train the classifiers for.\n",
    "\n",
    "Formulate the general problem as follows (See Wikipedia):\n",
    "\n",
    "Starts with a constant function $F_{0}(x)$ and some differentiable loss function $L$ and incrementally expands it in a greedy fashion:\n",
    "\n",
    "$$\n",
    "F_{0}(x)={\\underset {\\gamma }{\\arg \\min }}{\\sum _{i=1}^{n}{L(y_{i},\\gamma )}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_{m}(x)=F_{m-1}(x)+{\\underset {h_{m}\\in {\\mathcal {H}}}{\\operatorname {arg\\,min} }}\\left[{\\sum _{i=1}^{n}{L(y_{i},F_{m-1}(x_{i})+h_{m}(x_{i}))}}\\right]\n",
    "$$\n",
    "\n",
    "Finding the best $ h_{m}\\in {\\mathcal {H}}$ is computationally speaking impossible.\n",
    "If we could find the perfect $h$ however, we know that \n",
    "\n",
    "$$\n",
    "F_{m+1}(x_i)=F_{m}(x_i)+h(x_i)=y_i\n",
    "$$\n",
    "\n",
    "or, equivalently, \n",
    "\n",
    "$$\n",
    "   h(x_i)= y_i - F_{m}(x_i)\n",
    "$$\n",
    "\n",
    "Note that for the mean squared error loss $\\frac{1}{2}(y_i - F(x_i))^2$ this is equivalent to the negative \n",
    "gradient with respect to $F_i$.\n",
    "\n",
    "For a general loss function we fit $h_{m}(x)$ to the residuals, or negative gradients \n",
    "$$\n",
    " r_{i, m}=-\\left[{\\frac {\\partial L(y_{i},F(x_{i}))}{\\partial F(x_{i})}}\\right]_{F(x)=F_{m-1}(x)}\\quad {\\mbox{for }}i=1,\\ldots ,n.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Below we try it on CTA data again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:56:38.715276Z",
     "start_time": "2018-11-27T16:56:29.657159Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grb = GradientBoostingClassifier(\n",
    "    verbose=True,\n",
    "    n_estimators=300,\n",
    ")\n",
    "grb.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = grb.predict(X_test)\n",
    "y_prediction_proba = grb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:56:39.343691Z",
     "start_time": "2018-11-27T16:56:38.718176Z"
    }
   },
   "outputs": [],
   "source": [
    "l = [accuracy_score(y_pred, y_test) for y_pred in grb.staged_predict(X_test)]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(l)), l, '.')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:56:40.215880Z",
     "start_time": "2018-11-27T16:56:39.346460Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(fpr, tpr, c=thresholds)\n",
    "plt.plot(fpr, tpr, '--', color='gray', alpha=0.5)\n",
    "plt.text(0.5, 0.5, f'AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on gradient descent algorithms can be found in the Neural Network lecture.\n",
    "\n",
    "Let's now test our all time favorite classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:58:57.873659Z",
     "start_time": "2018-11-27T16:58:45.510336Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150,  max_depth=18, criterion='entropy')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rf.predict(X_test)\n",
    "y_prediction_proba = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:58:58.442111Z",
     "start_time": "2018-11-27T16:58:57.875736Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(fpr, tpr, c=thresholds)\n",
    "plt.plot(fpr, tpr, '--', color='gray', alpha=0.5)\n",
    "plt.text(0.5, 0.5, f'AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
